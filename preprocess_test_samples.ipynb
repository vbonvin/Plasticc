{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create samples from the test data\n",
    "\n",
    "The goal of this notebook is to make samples out of the test data provided by the organizers.\n",
    "\n",
    "As the test file is big (+6Gb, 3.5M objects), we will split the test data in smaller bunches that can be processed independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from util import *\n",
    "\n",
    "\n",
    "# load the data. Obviously this is waaay to big to be put in memory, so we read it chunck by chunck\n",
    "\n",
    "datapath = \"data/test_set.csv\"\n",
    "metadatapath = \"data/test_set_metadata.csv\"\n",
    "\n",
    "# df_chunks = pd.read_csv(datapath, chunksize=300)\n",
    "# dfm = pd.read_csv(metadatapath)\n",
    "\n",
    "# objs_ids = np.array(list(set(dfm[\"object_id\"])))\n",
    "# print(\"differents objects: \", len(objs_ids))\n",
    "# print(\"id_min: \", min(float(v) for v in objs_ids))\n",
    "# print(\"id_max: \", max(float(v) for v in objs_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have approx 3.5M objects. We'll put 10k objects per pickle, so ~350 pickles. Of course, the dumbfucks who created the challenge were not keen enough to label incrementally the ids...So we'll have to create a mapping function of our own\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize empty pickles\n",
    "\n",
    "nop = 10000 # Number of Objects per Pickle\n",
    "ids_chunks = [objs_ids[int(i*nop): int((i*nop)+10000)] for i in np.arange(len(objs_ids)/10000 + 1)]\n",
    "\n",
    "#print len(ids_chunks)\n",
    "#print len(list(set(ids_chunks[-1])))\n",
    "\n",
    "\n",
    "def map_to_pkl(id):\n",
    "\t\"\"\"Return the matching pickle index for the given object id\"\"\"\n",
    "\ti, = np.where(objs_ids == id)[0]\n",
    "\t#print(i)\n",
    "\tpi = int(i/nop)\n",
    "\treturn pi\n",
    "\n",
    "#print(map_to_pkl(max(float(v) for v in objs_ids)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import sys\n",
    "# df = dd.read_csv(datapath)\n",
    "\n",
    "# for chunk in pd.read_csv(datapath, chunksize=chunksize):\n",
    "#     for i in tqdm(np.arange(chunksize)):\n",
    "#         sample = chunk.iloc[i]\n",
    "#         id = sample[\"object_id\"]\n",
    "#         pickle_id = map_to_pkl(id)\n",
    "#     sys.exit()\n",
    "\n",
    "# start = time.time\n",
    "# for sample in df.itertuples():\n",
    "# \t#id = sample[\"object_id\"]\n",
    "# \t#pi = map_to_pkl(id)\n",
    "# \tpass\n",
    "# stop = time.time()\n",
    "# print(stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = dd.read_csv(\"data/chunks/test_set_chunk0.csv\")\n",
    "dfm = dd.read_csv(\"data/chunks/test_set_chunk0_metadata.csv\")\n",
    "\n",
    "# pdfm = pd.read_csv(\"data/chunks/test_set_chunk0_metadata.csv\")\n",
    "# objs_ids = np.array(list(set(pdfm[\"object_id\"])))\n",
    "# print(\"differents objects: \", len(objs_ids))\n",
    "# print(\"id_min: \", min(float(v) for v in objs_ids))\n",
    "# print(\"id_max: \", max(float(v) for v in objs_ids))\n",
    "# sys.exit()\n",
    "\n",
    "ids_df = df[\"object_id\"]\n",
    "ids_dfm = dfm[\"object_id\"]\n",
    "unique_ids = list(set(ids_dfm.compute().values))\n",
    "bands = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "#print(df_samples.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ddf     decl                                         detected_0  \\\n0   1 -4.63048  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1   1 -63.4483  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                          detected_1  \\\n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                          detected_2  \\\n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                          detected_3  \\\n0  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                          detected_4  \\\n0  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                          detected_5  distmod  \\\n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  43.9335   \n1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  43.1128   \n\n                                          fluxerrs_0   ...    hostgal_specz  \\\n0  [1.814225, 2.471813, 1.928534, 2.473823, 2.898...   ...              NaN   \n1  [1.949234, 1.901843, 2.72112, 3.228341, 3.0081...   ...              NaN   \n\n                                              mjds_0  \\\n0  [59818.274, 59819.2541, 59820.2522, 59821.2478...   \n1  [59819.1532, 59820.1047, 59821.1026, 59822.110...   \n\n                                              mjds_1  \\\n0  [59798.3281, 59801.3629, 59826.3181, 59842.253...   \n1  [59750.4306, 59752.4147, 59767.3045, 59770.225...   \n\n                                              mjds_2  \\\n0  [59798.3205, 59801.3553, 59826.3105, 59842.245...   \n1  [59750.4229, 59752.407, 59767.2968, 59770.2179...   \n\n                                              mjds_3  \\\n0  [59798.3357, 59801.3705, 59826.3258, 59842.260...   \n1  [59750.4383, 59752.4224, 59767.3122, 59770.233...   \n\n                                              mjds_4  \\\n0  [59798.3466, 59801.3815, 59826.3367, 59842.271...   \n1  [59750.445, 59752.4334, 59767.3233, 59770.2445...   \n\n                                              mjds_5  mwebv object_id       ra  \n0  [59798.3576, 59801.3924, 59826.3477, 59842.282...  0.022     98305  35.8594  \n1  [59752.4435, 59767.3343, 59770.2557, 59779.356...  0.021     98307  346.562  \n\n[2 rows x 35 columns]\nWrote test_samples_test.pkl\n"
     ]
    }
   ],
   "source": [
    "#sample = dfm_sample.compute().to_dict()\n",
    "\n",
    "#bmask = (df_samples[\"passband\"] == 0)\n",
    "#ss = df_samples[bmask][\"mjd\"]\n",
    "\n",
    "import time\n",
    "# start = time.time()\n",
    "# df_samples.compute()\n",
    "# \n",
    "# stop=time.time()\n",
    "# print(stop-start)\n",
    "\n",
    "# df_samples = df_samples.repartition(npartitions=1)\n",
    "# dfm_sample = dfm_sample.repartition(npartitions=1)\n",
    "# import time\n",
    "from tqdm import tqdm as tqdm\n",
    "series_list = []\n",
    "for u_id in unique_ids[:2]:\n",
    "\t#start = time.time()\t\n",
    "\tmask_df = (ids_df == u_id)\n",
    "\tdf_samples = df[mask_df]\n",
    "\n",
    "\tmask_dfm = (ids_dfm == u_id)\n",
    "\tdfm_sample = dfm[mask_dfm]\n",
    "\n",
    "\tdfm_sample = dfm_sample.compute().iloc[0].to_dict()\t\n",
    "\t#df_samples = df_samples.repartition(npartitions=1)\n",
    "\tdf_samples = df_samples.compute()\n",
    "\t\n",
    "\tfor b in bands:\n",
    "\t\tbmask = (df_samples[\"passband\"] == b)\n",
    "\t\tdfm_sample[\"mjds_%s\" % b] = df_samples[bmask][\"mjd\"].values\n",
    "\t\tdfm_sample[\"fluxes_%s\" % b] = df_samples[bmask][\"flux\"].values\n",
    "\t\tdfm_sample[\"fluxerrs_%s\" % b] = df_samples[bmask][\"flux_err\"].values\n",
    "\t\tdfm_sample[\"detected_%s\" % b] = df_samples[bmask][\"detected\"].values\n",
    "\t\n",
    "\t#stop = time.time()\n",
    "\t#print(stop-start)\t\n",
    "\tseries_list.append(pd.Series(dfm_sample))\n",
    "\t\n",
    "\t\n",
    "samples = pd.concat(series_list, axis=1).T\n",
    "# convert what needs to be integer as integer\n",
    "# ooh look Im pandas Im so fucking smart I save everything in float lolilol\n",
    "samples[[\"object_id\"]] = samples[[\"object_id\"]].astype(int)\t\n",
    "\t\n",
    "\t\n",
    "print(samples)\n",
    "# pickle the results for later use\n",
    "writepickle(samples, \"test_samples_test.pkl\")\n",
    "\t\n",
    "\t\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ddf     decl                                         detected_0  \\\n0   1 -4.63048  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1   1 -63.4483  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                          detected_1  \\\n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                          detected_2  \\\n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                          detected_3  \\\n0  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                          detected_4  \\\n0  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                          detected_5  distmod  \\\n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  43.9335   \n1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  43.1128   \n\n                                          fluxerrs_0   ...    hostgal_specz  \\\n0  [1.814225, 2.471813, 1.928534, 2.473823, 2.898...   ...              NaN   \n1  [1.949234, 1.901843, 2.72112, 3.228341, 3.0081...   ...              NaN   \n\n                                              mjds_0  \\\n0  [59818.274, 59819.2541, 59820.2522, 59821.2478...   \n1  [59819.1532, 59820.1047, 59821.1026, 59822.110...   \n\n                                              mjds_1  \\\n0  [59798.3281, 59801.3629, 59826.3181, 59842.253...   \n1  [59750.4306, 59752.4147, 59767.3045, 59770.225...   \n\n                                              mjds_2  \\\n0  [59798.3205, 59801.3553, 59826.3105, 59842.245...   \n1  [59750.4229, 59752.407, 59767.2968, 59770.2179...   \n\n                                              mjds_3  \\\n0  [59798.3357, 59801.3705, 59826.3258, 59842.260...   \n1  [59750.4383, 59752.4224, 59767.3122, 59770.233...   \n\n                                              mjds_4  \\\n0  [59798.3466, 59801.3815, 59826.3367, 59842.271...   \n1  [59750.445, 59752.4334, 59767.3233, 59770.2445...   \n\n                                              mjds_5  mwebv object_id       ra  \n0  [59798.3576, 59801.3924, 59826.3477, 59842.282...  0.022     98305  35.8594  \n1  [59752.4435, 59767.3343, 59770.2557, 59779.356...  0.021     98307  346.562  \n\n[2 rows x 35 columns]\nWrote test_samples_test.pkl\n"
     ]
    }
   ],
   "source": [
    "for b in bands:\n",
    "\tbmask = (df_samples[\"passband\"] == b)\n",
    "\tdfm_sample[\"mjds_%s\" % b] = df_samples[bmask][\"mjd\"].values\n",
    "\tdfm_sample[\"fluxes_%s\" % b] = df_samples[bmask][\"flux\"].values\n",
    "\tdfm_sample[\"fluxerrs_%s\" % b] = df_samples[bmask][\"flux_err\"].values\n",
    "\tdfm_sample[\"detected_%s\" % b] = df_samples[bmask][\"detected\"].values\n",
    "\t\n",
    "\t#dfm_sample.assign(df_samples[bmask][\"mjd\"])\n",
    "\t\n",
    "\t#sample[\"mjds_%s\" % b] = df_samples[bmask][\"mjd\"].compute().values\n",
    "\t#sample[\"fluxes_%s\" % b] \n",
    "\t#mjds_band = dd.from_array(np.random.permutation(len(df)))\n",
    "\t#dfm_sample.insert(loc=0, column=\"mjds_%s\" % b, values=df_samples[bmask][\"mjds_%s\" % b])\t\n",
    "print(dfm_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
